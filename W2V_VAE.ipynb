{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qara/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.slim import fully_connected as fc\n",
    "#from sklearn.manifold import TSNE\n",
    "\n",
    "from gensim.models import FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open('obj/' + name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def exist(name):\n",
    "    return os.path.exists('obj/' + name + '.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_obj('corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_mod = load_obj('bigram')\n",
    "trigram_mod = load_obj('trigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = FastText.load('embedding_reddit/embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qara/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/qara/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('murder', 1.0),\n",
       " ('convict', 0.9244711399078369),\n",
       " ('death', 0.912652850151062),\n",
       " ('arrest', 0.9031184315681458),\n",
       " ('charge', 0.9015302062034607),\n",
       " ('killing', 0.8916978240013123),\n",
       " ('murderer', 0.8909902572631836),\n",
       " ('guilty', 0.888261079788208),\n",
       " ('kidnapping', 0.8872666358947754),\n",
       " ('tortured', 0.8868398070335388)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.wv.similar_by_vector(embedding['murder'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = embedding.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29757\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mixmodel(object):\n",
    "    def __init__(self, n_z=4, sigma=1e-3, lr=1e-3, beta=1):\n",
    "        self.em_dim = 64\n",
    "        self.n_z = n_z\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "    \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.em_dim])\n",
    "        \n",
    "        self.z_mu, self.z_log_sigma_sq = self.vae_encoder(self.x)\n",
    "        eps = tf.random_normal(shape=tf.shape(self.z_log_sigma_sq),\n",
    "                               mean=0, stddev=self.sigma, dtype=tf.float32)\n",
    "        self.z = self.z_mu + tf.sqrt(tf.exp(self.z_log_sigma_sq)) * eps\n",
    "\n",
    "        self.x_recon = self.vae_decoder(self.z)\n",
    "        \n",
    "        ## VAE Loss ##\n",
    "        # Reconstruction loss\n",
    "        self.l2_loss = tf.reduce_mean(tf.squared_difference(self.x, self.x_recon))\n",
    "                \n",
    "        # Latent loss : Kullback Leibler divergence\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.z_log_sigma_sq - tf.square(self.z_mu) - tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "        self.latent_loss = self.beta * tf.reduce_mean(latent_loss)\n",
    "        \n",
    "        self.loss = self.l2_loss + self.latent_loss\n",
    "        #self.vae_train = tf.train.MomentumOptimizer(lr_2, 0.9, use_nesterov=True).minimize(self.vae_loss, var_list = vae_vars)\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss)\n",
    "    \n",
    "    def vae_encoder(self, x):\n",
    "        f1 = fc(x, 128, scope='vae_enc_fc1', activation_fn=tf.nn.elu)\n",
    "        #f1 = slim.batch_norm(f1, scope='vae_enc_fc1')\n",
    "        f2 = fc(f1, 128, scope='vae_enc_fc2', activation_fn=tf.nn.elu)\n",
    "        #f2 = slim.batch_norm(f2, scope='vae_enc_fc2')\n",
    "        f3 = fc(f2, 128, scope='vae_enc_fc3', activation_fn=tf.nn.elu)\n",
    "        #f3 = slim.batch_norm(f3, scope='vae_enc_fc3')\n",
    "        f4 = fc(f3, 64, scope='vae_enc_fc4', activation_fn=tf.nn.elu)\n",
    "        #f4 = slim.batch_norm(f4, scope='vae_enc_fc4')\n",
    "        f5 = fc(f4, 64, scope='vae_enc_fc5', activation_fn=tf.nn.elu)\n",
    "        #f5 = slim.batch_norm(f5, scope='vae_enc_fc5')\n",
    "        \n",
    "        z_mu = fc(f4, self.n_z, scope='vae_enc_fc11_mu', activation_fn=None)\n",
    "        z_log_sigma_sq = fc(f4, self.n_z, scope='vae_enc_fc11_sigma', activation_fn=None)\n",
    "        \n",
    "        return z_mu, z_log_sigma_sq\n",
    "    \n",
    "    def vae_decoder(self, z):\n",
    "        g6 = fc(z, 64, scope='vae_dec_fc6', activation_fn=tf.nn.elu)\n",
    "        #g6 = slim.batch_norm(g6, scope='vae_dec_fc6')\n",
    "        g7 = fc(g6, 64, scope='vae_dec_fc7', activation_fn=tf.nn.elu)\n",
    "        #g7 = slim.batch_norm(g7, scope='vae_dec_fc7')\n",
    "        g8 = fc(g7, 128, scope='vae_dec_fc8', activation_fn=tf.nn.elu)\n",
    "        #g8 = slim.batch_norm(g8, scope='vae_dec_fc8')\n",
    "        g9 = fc(g8, 128, scope='vae_dec_fc9', activation_fn=tf.nn.elu)\n",
    "        #g9 = slim.batch_norm(g9, scope='vae_dec_fc9')\n",
    "        g10 = fc(g9, 128, scope='vae_dec_fc10', activation_fn=tf.nn.elu)\n",
    "        #g10 = slim.batch_norm(g10, scope='vae_dec_fc10')\n",
    "        x_recon = fc(g10, self.em_dim, scope='vae_dec_fc11', activation_fn=None)\n",
    "        \n",
    "        return x_recon    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simplemodel(object):\n",
    "    def __init__(self, n_z=4, sigma=1e-3, lr=1e-3, beta=1):\n",
    "        self.em_dim = 100\n",
    "        self.n_z = n_z\n",
    "        self.beta = beta\n",
    "        self.sigma = sigma\n",
    "    \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.em_dim])\n",
    "        \n",
    "        self.z_mu, self.z_log_sigma_sq = self.vae_encoder(self.x)\n",
    "        eps = tf.random_normal(shape=tf.shape(self.z_log_sigma_sq),\n",
    "                               mean=0, stddev=self.sigma, dtype=tf.float32)\n",
    "        self.z = self.z_mu + tf.sqrt(tf.exp(self.z_log_sigma_sq)) * eps\n",
    "\n",
    "        self.x_recon = self.vae_decoder(self.z)\n",
    "        \n",
    "        ## VAE Loss ##\n",
    "        # Reconstruction loss\n",
    "        self.l2_loss = tf.reduce_mean(tf.squared_difference(self.x, self.x_recon))\n",
    "                \n",
    "        # Latent loss : Kullback Leibler divergence\n",
    "        latent_loss = -0.5 * tf.reduce_sum(\n",
    "            1 + self.z_log_sigma_sq - tf.square(self.z_mu) - tf.exp(self.z_log_sigma_sq), axis=1)\n",
    "        self.latent_loss = self.beta * tf.reduce_mean(latent_loss)\n",
    "        \n",
    "        self.loss = self.l2_loss + self.latent_loss\n",
    "        #self.vae_train = tf.train.MomentumOptimizer(lr_2, 0.9, use_nesterov=True).minimize(self.vae_loss, var_list = vae_vars)\n",
    "        self.train = tf.train.AdamOptimizer(learning_rate=lr).minimize(self.loss)\n",
    "    \n",
    "    def vae_encoder(self, x):\n",
    "        f1 = fc(x, 256, scope='vae_enc_fc1', activation_fn=tf.nn.elu)\n",
    "        #f1 = slim.batch_norm(f1, scope='vae_enc_fc1')\n",
    "        f2 = fc(f1, 256, scope='vae_enc_fc2', activation_fn=tf.nn.elu)\n",
    "        #f2 = slim.batch_norm(f2, scope='vae_enc_fc2')\n",
    "        \n",
    "        z_mu = fc(f2, self.n_z, scope='vae_enc_fc11_mu', activation_fn=None)\n",
    "        z_log_sigma_sq = fc(f2, self.n_z, scope='vae_enc_fc11_sigma', activation_fn=None)\n",
    "        \n",
    "        return z_mu, z_log_sigma_sq\n",
    "    \n",
    "    def vae_decoder(self, z):\n",
    "        g9 = fc(z, 256, scope='vae_dec_fc9', activation_fn=tf.nn.elu)\n",
    "        #g9 = slim.batch_norm(g9, scope='vae_dec_fc9')\n",
    "        g10 = fc(g9, 256, scope='vae_dec_fc10', activation_fn=tf.nn.elu)\n",
    "        #g10 = slim.batch_norm(g10, scope='vae_dec_fc10')\n",
    "        x_recon = fc(g10, 100, scope='vae_dec_fc11', activation_fn=None)\n",
    "        \n",
    "        return x_recon    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#model = simplemodel(n_z=64, sigma=0, lr=1e-3, beta=1)\n",
    "model = mixmodel(n_z=16, sigma=0, lr=1e-3, beta=1)\n",
    "\n",
    "tfconfig = tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth=True))\n",
    "sess=tf.Session(config=tfconfig)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt_w2v/vae16_100.ckpt\n"
     ]
    }
   ],
   "source": [
    "model_name = 'vae16_100'\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, './ckpt_w2v/%s.ckpt' %model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/100]\n",
      "total loss: <0.1500>, l2: 0.0919, latent: 0.0580\n",
      "Epoch: [2/100]\n",
      "total loss: <0.0714>, l2: 0.0637, latent: 0.0077\n",
      "Epoch: [3/100]\n",
      "total loss: <0.0612>, l2: 0.0558, latent: 0.0054\n",
      "Epoch: [4/100]\n",
      "total loss: <0.0549>, l2: 0.0507, latent: 0.0042\n",
      "Epoch: [5/100]\n",
      "total loss: <0.0510>, l2: 0.0475, latent: 0.0035\n",
      "Epoch: [6/100]\n",
      "total loss: <0.0478>, l2: 0.0449, latent: 0.0029\n",
      "Epoch: [7/100]\n",
      "total loss: <0.0462>, l2: 0.0439, latent: 0.0023\n",
      "Epoch: [8/100]\n",
      "total loss: <0.0440>, l2: 0.0420, latent: 0.0020\n",
      "Epoch: [9/100]\n",
      "total loss: <0.0425>, l2: 0.0408, latent: 0.0017\n",
      "Epoch: [10/100]\n",
      "total loss: <0.0411>, l2: 0.0397, latent: 0.0014\n",
      "Epoch: [11/100]\n",
      "total loss: <0.0406>, l2: 0.0394, latent: 0.0012\n",
      "Epoch: [12/100]\n",
      "total loss: <0.0400>, l2: 0.0390, latent: 0.0010\n",
      "Epoch: [13/100]\n",
      "total loss: <0.0396>, l2: 0.0388, latent: 0.0009\n",
      "Epoch: [14/100]\n",
      "total loss: <0.0394>, l2: 0.0386, latent: 0.0008\n",
      "Epoch: [15/100]\n",
      "total loss: <0.0392>, l2: 0.0384, latent: 0.0007\n",
      "Epoch: [16/100]\n",
      "total loss: <0.0390>, l2: 0.0383, latent: 0.0007\n",
      "Epoch: [17/100]\n",
      "total loss: <0.0388>, l2: 0.0382, latent: 0.0007\n",
      "Epoch: [18/100]\n",
      "total loss: <0.0386>, l2: 0.0379, latent: 0.0006\n",
      "Epoch: [19/100]\n",
      "total loss: <0.0385>, l2: 0.0379, latent: 0.0006\n",
      "Epoch: [20/100]\n",
      "total loss: <0.0383>, l2: 0.0377, latent: 0.0006\n",
      "Epoch: [21/100]\n",
      "total loss: <0.0382>, l2: 0.0375, latent: 0.0007\n",
      "Epoch: [22/100]\n",
      "total loss: <0.0380>, l2: 0.0373, latent: 0.0007\n",
      "Epoch: [23/100]\n",
      "total loss: <0.0378>, l2: 0.0371, latent: 0.0007\n",
      "Epoch: [24/100]\n",
      "total loss: <0.0376>, l2: 0.0369, latent: 0.0007\n",
      "Epoch: [25/100]\n",
      "total loss: <0.0374>, l2: 0.0367, latent: 0.0007\n",
      "Epoch: [26/100]\n",
      "total loss: <0.0371>, l2: 0.0364, latent: 0.0007\n",
      "Epoch: [27/100]\n",
      "total loss: <0.0368>, l2: 0.0361, latent: 0.0007\n",
      "Epoch: [28/100]\n",
      "total loss: <0.0367>, l2: 0.0360, latent: 0.0007\n",
      "Epoch: [29/100]\n",
      "total loss: <0.0366>, l2: 0.0359, latent: 0.0007\n",
      "Epoch: [30/100]\n",
      "total loss: <0.0362>, l2: 0.0355, latent: 0.0007\n",
      "Epoch: [31/100]\n",
      "total loss: <0.0360>, l2: 0.0354, latent: 0.0007\n",
      "Epoch: [32/100]\n",
      "total loss: <0.0358>, l2: 0.0352, latent: 0.0006\n",
      "Epoch: [33/100]\n",
      "total loss: <0.0357>, l2: 0.0350, latent: 0.0006\n",
      "Epoch: [34/100]\n",
      "total loss: <0.0355>, l2: 0.0349, latent: 0.0006\n",
      "Epoch: [35/100]\n",
      "total loss: <0.0354>, l2: 0.0348, latent: 0.0006\n",
      "Epoch: [36/100]\n",
      "total loss: <0.0352>, l2: 0.0346, latent: 0.0006\n",
      "Epoch: [37/100]\n",
      "total loss: <0.0351>, l2: 0.0345, latent: 0.0006\n",
      "Epoch: [38/100]\n",
      "total loss: <0.0349>, l2: 0.0343, latent: 0.0006\n",
      "Epoch: [39/100]\n",
      "total loss: <0.0348>, l2: 0.0342, latent: 0.0006\n",
      "Epoch: [40/100]\n",
      "total loss: <0.0346>, l2: 0.0341, latent: 0.0006\n",
      "Epoch: [41/100]\n",
      "total loss: <0.0346>, l2: 0.0340, latent: 0.0006\n",
      "Epoch: [42/100]\n",
      "total loss: <0.0345>, l2: 0.0339, latent: 0.0006\n",
      "Epoch: [43/100]\n",
      "total loss: <0.0344>, l2: 0.0338, latent: 0.0006\n",
      "Epoch: [44/100]\n",
      "total loss: <0.0342>, l2: 0.0336, latent: 0.0006\n",
      "Epoch: [45/100]\n",
      "total loss: <0.0341>, l2: 0.0335, latent: 0.0006\n",
      "Epoch: [46/100]\n",
      "total loss: <0.0340>, l2: 0.0334, latent: 0.0005\n",
      "Epoch: [47/100]\n",
      "total loss: <0.0338>, l2: 0.0333, latent: 0.0005\n",
      "Epoch: [48/100]\n",
      "total loss: <0.0338>, l2: 0.0332, latent: 0.0005\n",
      "Epoch: [49/100]\n",
      "total loss: <0.0336>, l2: 0.0331, latent: 0.0005\n",
      "Epoch: [50/100]\n",
      "total loss: <0.0336>, l2: 0.0330, latent: 0.0005\n",
      "Epoch: [51/100]\n",
      "total loss: <0.0334>, l2: 0.0329, latent: 0.0005\n",
      "Epoch: [52/100]\n",
      "total loss: <0.0333>, l2: 0.0328, latent: 0.0005\n",
      "Epoch: [53/100]\n",
      "total loss: <0.0332>, l2: 0.0327, latent: 0.0005\n",
      "Epoch: [54/100]\n",
      "total loss: <0.0331>, l2: 0.0326, latent: 0.0005\n",
      "Epoch: [55/100]\n",
      "total loss: <0.0330>, l2: 0.0325, latent: 0.0005\n",
      "Epoch: [56/100]\n",
      "total loss: <0.0329>, l2: 0.0324, latent: 0.0005\n",
      "Epoch: [57/100]\n",
      "total loss: <0.0328>, l2: 0.0323, latent: 0.0005\n",
      "Epoch: [58/100]\n",
      "total loss: <0.0327>, l2: 0.0322, latent: 0.0005\n",
      "Epoch: [59/100]\n",
      "total loss: <0.0326>, l2: 0.0321, latent: 0.0005\n",
      "Epoch: [60/100]\n",
      "total loss: <0.0325>, l2: 0.0320, latent: 0.0005\n",
      "Epoch: [61/100]\n",
      "total loss: <0.0324>, l2: 0.0319, latent: 0.0005\n",
      "Epoch: [62/100]\n",
      "total loss: <0.0324>, l2: 0.0319, latent: 0.0005\n",
      "Epoch: [63/100]\n",
      "total loss: <0.0322>, l2: 0.0318, latent: 0.0005\n",
      "Epoch: [64/100]\n",
      "total loss: <0.0322>, l2: 0.0317, latent: 0.0005\n",
      "Epoch: [65/100]\n",
      "total loss: <0.0321>, l2: 0.0317, latent: 0.0005\n",
      "Epoch: [66/100]\n",
      "total loss: <0.0320>, l2: 0.0316, latent: 0.0005\n",
      "Epoch: [67/100]\n",
      "total loss: <0.0320>, l2: 0.0316, latent: 0.0005\n",
      "Epoch: [68/100]\n",
      "total loss: <0.0319>, l2: 0.0314, latent: 0.0005\n",
      "Epoch: [69/100]\n",
      "total loss: <0.0318>, l2: 0.0313, latent: 0.0004\n",
      "Epoch: [70/100]\n",
      "total loss: <0.0317>, l2: 0.0312, latent: 0.0004\n",
      "Epoch: [71/100]\n",
      "total loss: <0.0316>, l2: 0.0312, latent: 0.0004\n",
      "Epoch: [72/100]\n",
      "total loss: <0.0316>, l2: 0.0311, latent: 0.0004\n",
      "Epoch: [73/100]\n",
      "total loss: <0.0314>, l2: 0.0310, latent: 0.0004\n",
      "Epoch: [74/100]\n",
      "total loss: <0.0314>, l2: 0.0310, latent: 0.0004\n",
      "Epoch: [75/100]\n",
      "total loss: <0.0314>, l2: 0.0309, latent: 0.0004\n",
      "Epoch: [76/100]\n",
      "total loss: <0.0313>, l2: 0.0309, latent: 0.0004\n",
      "Epoch: [77/100]\n",
      "total loss: <0.0312>, l2: 0.0308, latent: 0.0004\n",
      "Epoch: [78/100]\n",
      "total loss: <0.0312>, l2: 0.0307, latent: 0.0004\n",
      "Epoch: [79/100]\n",
      "total loss: <0.0310>, l2: 0.0306, latent: 0.0004\n",
      "Epoch: [80/100]\n",
      "total loss: <0.0310>, l2: 0.0306, latent: 0.0004\n",
      "Epoch: [81/100]\n",
      "total loss: <0.0309>, l2: 0.0305, latent: 0.0004\n",
      "Epoch: [82/100]\n",
      "total loss: <0.0309>, l2: 0.0305, latent: 0.0004\n",
      "Epoch: [83/100]\n",
      "total loss: <0.0307>, l2: 0.0303, latent: 0.0004\n",
      "Epoch: [84/100]\n",
      "total loss: <0.0308>, l2: 0.0304, latent: 0.0004\n",
      "Epoch: [85/100]\n",
      "total loss: <0.0307>, l2: 0.0303, latent: 0.0004\n",
      "Epoch: [86/100]\n",
      "total loss: <0.0307>, l2: 0.0303, latent: 0.0004\n",
      "Epoch: [87/100]\n",
      "total loss: <0.0306>, l2: 0.0302, latent: 0.0004\n",
      "Epoch: [88/100]\n",
      "total loss: <0.0306>, l2: 0.0302, latent: 0.0004\n",
      "Epoch: [89/100]\n",
      "total loss: <0.0305>, l2: 0.0301, latent: 0.0004\n",
      "Epoch: [90/100]\n",
      "total loss: <0.0304>, l2: 0.0300, latent: 0.0004\n",
      "Epoch: [91/100]\n",
      "total loss: <0.0304>, l2: 0.0300, latent: 0.0004\n",
      "Epoch: [92/100]\n",
      "total loss: <0.0303>, l2: 0.0299, latent: 0.0004\n",
      "Epoch: [93/100]\n",
      "total loss: <0.0303>, l2: 0.0299, latent: 0.0004\n",
      "Epoch: [94/100]\n",
      "total loss: <0.0302>, l2: 0.0298, latent: 0.0004\n",
      "Epoch: [95/100]\n",
      "total loss: <0.0302>, l2: 0.0298, latent: 0.0004\n",
      "Epoch: [96/100]\n",
      "total loss: <0.0301>, l2: 0.0297, latent: 0.0004\n",
      "Epoch: [97/100]\n",
      "total loss: <0.0301>, l2: 0.0297, latent: 0.0004\n",
      "Epoch: [98/100]\n",
      "total loss: <0.0300>, l2: 0.0296, latent: 0.0004\n",
      "Epoch: [99/100]\n",
      "total loss: <0.0300>, l2: 0.0296, latent: 0.0004\n",
      "Epoch: [100/100]\n",
      "total loss: <0.0299>, l2: 0.0296, latent: 0.0004\n"
     ]
    }
   ],
   "source": [
    "bs = 100\n",
    "num_steps = len(train_data)//bs\n",
    "ne = 100\n",
    "print_step = 400\n",
    "save_step = 5\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep = 10)\n",
    "\n",
    "for epoch in range(ne):\n",
    "    print('Epoch: [%d/%d]' %(epoch+1, ne))\n",
    "    count = 0 \n",
    "    avg_loss = 0\n",
    "    avg_l2_loss = 0\n",
    "    avg_latent_loss = 0\n",
    "    \n",
    "    np.random.shuffle(train_data)\n",
    "    for idx in range(num_steps):\n",
    "        batch_x = train_data[idx*bs:(idx+1)*bs]\n",
    "        loss, l2_loss, latent_loss, _ = sess.run([model.loss, \\\n",
    "                model.l2_loss, model.latent_loss, model.train], \\\n",
    "                feed_dict = {model.x: batch_x})\n",
    "\n",
    "        count += 1        \n",
    "        avg_loss += loss\n",
    "        avg_l2_loss += l2_loss\n",
    "        avg_latent_loss += latent_loss\n",
    "\n",
    "    print('total loss: <%.4f>, l2: %.4f, latent: %.4f' \\\n",
    "         %(avg_loss/count, avg_l2_loss/count, avg_latent_loss/count))\n",
    "    \n",
    "    if (epoch+1)%save_step==0:\n",
    "        saver.save(sess, './ckpt_w2v/vae32_%d.ckpt' %(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00559954 -0.00951704  0.00137021  0.00206775  0.00935208 -0.00566384\n",
      "  0.00708273  0.00113553 -0.01599122 -0.01370033  0.00104946  0.00606643\n",
      " -0.0087571  -0.00569791 -0.00328011 -0.00063683]\n",
      "['president', 'leader', 'preside', 'biden', 'opponent', 'barack_obama', 'ouster', 'oust', 'dictatorship', 'successor']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qara/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/home/qara/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "word = 'president'\n",
    "word_vec = embedding[word]\n",
    "code, recon_vec = sess.run([model.z, model.x_recon], feed_dict={model.x: [word_vec]})\n",
    "print(code[0])\n",
    "\n",
    "similar = embedding.wv.similar_by_vector(recon_vec[0])\n",
    "similar_words, similarities = zip(*similar)\n",
    "print(list(similar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_by_distance(target_vec, num_out=10):\n",
    "    distance = np.sum(np.power(embedding.wv.vectors - target_vec, 2), axis=1)\n",
    "    idx = np.argpartition(distance, num_out)\n",
    "    result = []\n",
    "    for i in range(num_out):\n",
    "        result.append(embedding.wv.index2word[idx[i]])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qara/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['wound',\n",
       " 'injured',\n",
       " 'dozen',\n",
       " 'dead',\n",
       " 'least',\n",
       " 'injure',\n",
       " 'critically_injur',\n",
       " 'wounded',\n",
       " 'kill',\n",
       " 'seriously_injur']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_by_distance(embedding['injure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qara/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('injure', 1.0),\n",
       " ('injured', 0.9605617523193359),\n",
       " ('wound', 0.9437973499298096),\n",
       " ('wounded', 0.9277883172035217),\n",
       " ('least', 0.9168642163276672),\n",
       " ('dozen', 0.904802143573761),\n",
       " ('kill', 0.8982696533203125),\n",
       " ('dead', 0.8954891562461853),\n",
       " ('critically_injur', 0.8931788206100464),\n",
       " ('trains_collide', 0.8898150324821472)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.wv.similar_by_vector(embedding.wv['injure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stock', 1.0), ('investor', 0.7777920961380005), ('stock_market', 0.7727078199386597), ('appetite', 0.7531195878982544), ('trading', 0.74650639295578), ('pile', 0.7432770729064941), ('emerging_market', 0.7402875423431396), ('bond', 0.7398805618286133), ('commodity', 0.7372817397117615), ('profit', 0.7363636493682861)]\n",
      "\n",
      "['stock', 'bond', 'dollar', 'price', 'buying', 'appetite', 'dip', 'profit', 'boon', 'cash']\n",
      "\n",
      "['instance', 'profit', 'stock', 'appetite', 'trouble', 'dollar', 'dip', 'value', 'bond', 'buying']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qara/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "word = 'stock'\n",
    "word_vec = embedding.wv[word]\n",
    "code, recon_vec = sess.run([model.z, model.x_recon], feed_dict={model.x: [word_vec]})\n",
    "#print(recon_vec[0])\n",
    "\n",
    "similar = embedding.wv.similar_by_vector(word_vec)\n",
    "similar_words, similarities = zip(*similar)\n",
    "#print(list(similar_words))\n",
    "print(similar)\n",
    "\n",
    "similar = embedding.wv.similar_by_vector(recon_vec[0])\n",
    "similar_words, similarities = zip(*similar)\n",
    "print()\n",
    "print(list(similar_words))\n",
    "print()\n",
    "print(similar_by_distance(recon_vec[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
